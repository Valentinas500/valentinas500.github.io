<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Introductory Econometrics. Chapter 6</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Valentinas Rudys</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="teaching.html">Teaching</a>
</li>
<li>
  <a href="research.html">Research</a>
</li>
<li>
  <a href="etrics.html">Econometrics</a>
</li>
<li>
  <a href="macro.html">Macroeconomics</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="mailto:vrudys@fordham.edu">
    <span class="fa fa-envelope-o"></span>
     
    Contact me
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Introductory Econometrics. Chapter 6</h1>

</div>


<style>
p.comment {
background-color: #e8e8e8;
padding: 10px;
border: 0px solid black;
margin-left: 25px;
border-radius: 5px;
}
</style>
<div id="chapter-6-the-multiple-regression-analysis-further-issues" class="section level5">
<h5>Chapter 6: The Multiple Regression Analysis: Further Issues</h5>
<p>We sometimes are interested in changing the units of our variables, say from feet to meters or pounds to kilograms. Unsurprisingly, when variables are re-scaled, the coefficients, standard errors, confidence intervals, t statistics and F statistics change in ways that preserve all measured effects and testing outcomes. However, data scaling can be done for cosmetic purposes: to make the estimated coefficients easier to read and interpret. For example, if the unit of measurement is very large and the estimated coefficient is very small, it may be convenient to rescale data to reduce the number of zeros after the decimal point.</p>
<p>In some cases, it is easier to interpret the results not in actual test scores or whatever numbers it is measured by in standard deviations. To do that, we need to standardize a variable. A variable is standardized in the sample by subtracting off its mean and dividing by its standard deviation. We simply compute the z-score for every variable in the sample. When, we run the regression using the z-scores. Estimated coefficients obtained from a regression using standardized variables are called standardized coefficients or beta coefficients. These coefficients meaning can be explained as follows: if X1 increases by one standard deviation, then Y changes by b1^hat standard deviations. When all variables are standardized, we can compare the beta coefficient magnitudes. It can also be useful to standardize variables when for example there is large variation in one independent variable but little variation in another independent variable.</p>
<p>Besides work with data, we need to be able to understand the differences between different functional forms. First, the logarithmic functional forms provide a convenient percentage (log-level) and elasticity (log-log) interpretation. Slope coefficients of logged variables are invariant to rescalings. Also, taking natural logarithms often mitigates the outlier problems and helps secure normality and homoskedasticity. Logs are often used working with dollar amounts such as wage or income.</p>
<p>When you shouldn’t use logs? We usually do not use log for variables reporting percentage rates. Whenever variables are measured in time units such as years, we cannot use logs. We also cannot take natural logs of zero or negative values. If the values are very small, we should probably stay away from logs as they can increase the variation as log around small values creates large differences.</p>
<p>Note that when one uses a logarithmic dependent variable, variation in our independent variables explain the variation in the logarithmic dependent variable while we are interested how they affect variation in our dependent variable, not its log. We need to not forget to use the exponent correctly to reverse the log operation. We will need to use an additional variable a0 will have to be estimated and used as shown at the end of this chapter.</p>
<p>Also, when using log-level form, to get the exact percentage change that <span class="math inline">\(y\)</span> is affected after <span class="math inline">\(x\)</span> increases by 1 unit, we can use the following formula:</p>
<p><span class="math display">\[ \% \Delta \hat y = 100 [\exp(\hat \beta_2 \Delta x_1)-1] \quad \rightarrow \quad \% \Delta \hat y = 100[\exp(\hat \beta_2) - 1] \text{ when } \Delta x_1 = 1  \]</span></p>
<p>Quadratic form is another very popular function form that is able to capture nonlinear effects. You can measure either increasing or decreasing returns to education, or the return to experience. We may expect that the first few years of experience will have a larger effect on your wage than a few additional years when you already amassed a lot of experience.</p>
<p>When the slopes of the level variable of interest and its squared are of different sign, we may be interested in computing the turning point. It estimates when the overall effect on the explained variable switches the sign. However, it does not mean that turning point is useful. In some cases, we may find that an increase in education beyond some point may reduce the wage. Before we conclude that we should check how many sample observations lie to the right of the turning point: likely none to a few. We may have a specification issue.</p>
<p>If both slope coefficients are of the same sign, than there is no such turning point, and the increase (if both positive) or the decrease (if both negative) in the dependent variable keeps getting larger as the independent variable keeps increasing at higher level.</p>
<p>One can try even higher polynomials, for example, we can run a regression using experience, experience-squared and/or experience-cubed. Similarly, a firm may find that their cost function is best described by a higher polynomial. A more careful analysis should be done before making a decision to use such a form.</p>
<div class="figure">
<img src="1114444.png" />

</div>
<p>Let’s work out the wage example again using both experience and experience squared as regressors. Our regression model is: <span class="math display">\[wage = \beta_0 + \beta_1*exper + \beta_2*(exper)^2\]</span></p>
<p>In R, write the following code:</p>
<pre class="r"><code>data(wage1, package=&#39;wooldridge&#39;)
attach(wage1)
reg1=lm(wage~exper+expersq)
plot(wage~exper)
x=seq(from=1, to=51, by=1)
pred=predict(reg1, data.frame(exper=x, expersq=x^2), interval = &quot;confidence&quot;)
lines(x, pred[,1])</code></pre>
<p><img src="etricsCh6_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>which(max(pred[,1])==pred[,1])</code></pre>
<pre><code>## 24 
## 24</code></pre>
<pre class="r"><code>maxvalue=abs(reg1$coefficients[2]/2/reg1$coefficients[3])</code></pre>
<p>The results are shown below. Using the formula to find the maximum value of a quadratic function, we can find that we predict the maximum wage is reached when experience is 24.4. However, this is very unlikely to be true. It is more likely that we have a specification issue, we are missing some key explanatory variables.</p>
<div class="figure">
<img src="1111111.png" />

</div>
<div class="figure">
<img src="11111222.png" />

</div>
<p>In some cases, the partial effect of one variable may depend on the level of the other variable. For example, the return to experience may be different for someone with college degree and without, or the number of bedrooms has a different effect when the square footage of the house is larger. In these cases, we use interaction terms. We basically state that number of bedrooms has a different effect depending on square footage. We need to be careful when interpreting results in such cases.</p>
<p>Let’s consider an example in which we use an interaction term between square feet and number of bedrooms. The full regression model looks like this: <span class="math display">\[price = \beta_0 + \beta_1*sqrft + \beta_2*bdrms + \beta_3*sqrft*bdrms + \beta_4*bthrms\]</span> Say, we are interested in the effect of number of bedrooms on the price of a house. We see that the change in price due to change in 1 bedroom depends on the square footage of the house. We may choose to use a specific value, for example, the mean square footage, to get a point estimate. In this case, we will call this average partial effect (APE) of bedrooms on the price.</p>
<div class="figure">
<img src="111115555.png" />

</div>
<p>Often, it is useful to reparametrize the model so that the slope coefficients on the original variables have an interesting meaning. For example, instead of simply using interaction term between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, we can use an interaction term <span class="math inline">\((x_1-\mu_1)(\x_2-\mu_2)\)</span> where <span class="math inline">\(\mu_1\)</span> is the mean of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(\mu_2\)</span> is the mean of <span class="math inline">\(x_2\)</span>. Now, the non-interactive terms have a simple interpretation. <span class="math inline">\(\delta_2\)</span> now measures the effect of <span class="math inline">\(x_2\)</span> on <span class="math inline">\(y\)</span> when variables take their mean values. It is much easier to interpret directly (without doing any additional math operations) and we get the standard errors for partial effects at the mean values. If necessary, we can center the interaction term not at their means but any other interesting values.</p>
<p>In models with quadratics, interactions, and other nonlinear functional forms, the partial effect depends on the values of one or more explanatory variables. However, we often want a single value to describe the relationship between the dependent variable and each explanatory variable. One popular summary measure is the average partial effect (APE), sometimes called average marginal effect. To get APE, we use the sample average of the variables in the estimated partial effect formula.</p>
<p>For example, if we measure the partial effect of education to be <span class="math inline">\(\beta_1+\beta_2*(experience)\)</span>, then <span class="math inline">\(APE=\beta_1+\beta_2*(mean(experience))\)</span></p>
<p>Nothing about the classical linear model assumptions requires that R-squared be above a a certain value. A high R-squared does not imply that there is a causal interpretation. A low R-squared does not preclude precise estimation of partial effects. Choosing a set of explanatory variables based on the size of resulting R-squared can lead to nonsensical models.</p>
<p>The problem with R-squared is that R-squared never decreases when we add additional explanatory variables. Adjusted, R-squared, on the other hand imposes a penalty for adding additional independent variables. The adjusted R-squared increases if, and only if, the t-statistic of a newly added regressor is greater than one in absolute value. Adjusted R-squared is also usually reported in most econometric software.</p>
<div class="figure">
<img src="111188888.png" />

</div>
<p>Comparing R-squared is not correct if the number of explanatory variables is not the same. However, we can compare the adjusted R-squared because it takes into account the difference in degrees of freedom (it penalizes us for adding more explanatory variables).</p>
<p>Comparing R-square or adjusted R-squared is incorrect when we are using dependent variables that differ in their definition. For example, if we use the same independent variables to explain salary and log(salary), R-squared and adjusted R-squared should not be compared. Model with log(salary) has much less variation that needs to be explained. We expect both R-squared and adjusted R-squared to be significantly lower when we use log(salary) instead of salary.</p>
<div class="figure">
<img src="112233.png" />

</div>
<p>Previously, we have discussed that if we omit a relevant independent variable, we may have omitted variable bias. However, that does not mean we should try to control for everything we have data on. Controlling for too many factors may be a problem in regression analysis as well. In some cases, certain variables should not be held fixed. For example, in a regression of traffic fatalities on state beer taxes (and other factors) one should not directly control for beer consumption. If one included beer consumption, then there is no link between fatalities and taxes. Similarly, in a regression of family health expenditures on pesticide usage among farmers one should not control for doctor visits. Health expenditures already include doctor visits. If we included doctor visits as an independent variable, then we would be measuring the pesticide use on health expenditures other than doctors visits.</p>
<p>Different regressions may serve different purposes. In a regression of house prices on house characteristics, one would only include price assessments if the purpose of the regression is to study their validity; otherwise one would not include them.</p>
<p>We discussed earlier that adding regressors may exacerbate multicollinearity problems. On the other hand, adding regressors reduces the error variance. Variables that are uncorrelated with other regressors should be added because they reduce error variance without increasing multicollinearity. However, such uncorrelated variables may be hard to find. As you will see, most variables are correlated in some way. Adding variables to your regression model must be well thought out.</p>
<p>Often, we want to measure the uncertainty of our predicted values and we can use confidence interval for that. For example, say we are interested in predicting college GPA for specific given information.</p>
<pre class="r"><code>data(gpa2, package=&#39;wooldridge&#39;)
reg = lm(colgpa~sat+hsperc+hsize+I(hsize^2),data=gpa2)
cvalues1 = data.frame(sat=1200, hsperc=30, hsize=5) # Generate data for predictions
p1=predict(reg, cvalues1); p1 # Point estimate of prediction</code></pre>
<pre><code>##        1 
## 2.700075</code></pre>
<pre class="r"><code>p2=predict(reg, cvalues1, interval = &quot;confidence&quot;); p2 # Point estimate and 95% confidence interval</code></pre>
<pre><code>##        fit      lwr      upr
## 1 2.700075 2.661104 2.739047</code></pre>
<pre class="r"><code>satrange=seq(from=500, to=1600, by=1)
cvalues2 = data.frame(sat=satrange, hsperc=30, hsize=5) # Generate new data
predGPA=predict(reg, cvalues2, interval = &quot;confidence&quot;)
plot(predGPA[,1]~satrange, type=&quot;l&quot;)
lines(predGPA[,2]~satrange, lty=2)
lines(predGPA[,3]~satrange, lty=2)</code></pre>
<p><img src="etricsCh6_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>plot(reg$residuals~gpa2$sat)</code></pre>
<p><img src="etricsCh6_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<p>After executing p1, we see that the predicted college GPA for someone who has scored 1200 on SAT, was in 30th percentile in high school, and is from a household of 5, is 2.70.</p>
<p>What if we want to get 95% confidence interval for that value? p2 computes the point estimate and the lower bound (2.66) and upper bound (2.74) for the predicted GPA.</p>
<p>Running the rest of the code plots the regression line given the information on high school percentile and household size. We see that the confidence interval increases as we go away from the mean values.</p>
<p>Residual analysis is another tool that we can use to better understand how good are our predictions. The difference between the actual observations and the predicted values are residuals. To continue our last example, just add the following line to see the residuals plotted against the SAT scores.</p>
<p>Predicting the dependent variable when we use non-level form of the dependent variable in the regression model is not direct, although still quite simple. While it would seem that it is enough to simply un-log (using the exponent), there is a small adjustment that needs to be done.</p>
<p><span class="math inline">\(\hat y = \hat \alpha_0 \exp(\widehat{\log y})\)</span> where $<em>0=n^{-1}</em>{i=1}^n (u_i) $</p>
<p>See CEO salary example below.</p>
<pre class="r"><code>data(ceosal2, package=&#39;wooldridge&#39;)
reg1=lm(log(salary)~log(sales)+log(mktval)+ceoten, data=ceosal2)
a0=1/length(fitted(reg1)) * sum(exp(reg1$residuals))
sales_range=seq(from=min(ceosal2$sales),to=max(ceosal2$sales),by=200)
cvalues = data.frame(sales=sales_range, mktval=mean(ceosal2$mktval), ceoten=mean(ceosal2$ceoten)) 
logpred=predict(reg1, cvalues, interval = &quot;confidence&quot;)
PredictedSalary=a0*exp(logpred)
plot(PredictedSalary[,1]~sales_range, type=&quot;l&quot;)
lines(PredictedSalary[,2]~sales_range, type=&quot;l&quot;, lty=2)
lines(PredictedSalary[,3]~sales_range, type=&quot;l&quot;, lty=2)</code></pre>
<p><img src="etricsCh6_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p><strong>Homework Problems</strong></p>
<p class="comment">
Computer Exercise C1.<br />
Use the data in <strong>kielmc</strong>, only for the year 1981, to answer the following questions. The data are for houses that sold during 1981 in North Andover, Massachusetts; 1981 was the year construction began on a local garbage incinerator.<br />
1. To study the effects of the incinerator location on housing price, consider the simple regression model:<span class="math display">\[ log(price) = \beta_0 + \beta_1*log(dist) + u \]</span> where price is housing price in dollars and dist is distance from the house to the incinerator measured in feet. Interpreting this equation causally, what sign do you expect for <span class="math inline">\(\beta_1\)</span> if the presence of the incinerator depresses housing prices? Estimate this equation and interpret the results.<br />
2. To the simple regression model in part 1, add the variables <span class="math inline">\(log(intst)\)</span>, <span class="math inline">\(log(area)\)</span>, <span class="math inline">\(log(land)\)</span>, <span class="math inline">\(rooms\)</span>, <span class="math inline">\(baths\)</span>, and <span class="math inline">\(age\)</span>, where <span class="math inline">\(intst\)</span> is distance from the home to the interstate, <span class="math inline">\(area\)</span> is square footage of the house, <span class="math inline">\(land\)</span> is the lot size in square feet, <span class="math inline">\(rooms\)</span> is total number of rooms, <span class="math inline">\(baths\)</span> is number of bathrooms, and <span class="math inline">\(age\)</span> is age of the house in years. Now, what do you conclude about the effects of the incinerator? Explain why part 1 and part 2 give conflicting results.<br />
3. Add <span class="math inline">\([log(intst)]^2\)</span> to the model from part 2. Now what happens? What do you conclude about the importance of functional form?<br />
4. Is the square of <span class="math inline">\(log(dist)\)</span> significant when you add it to the model from part 3?
</p>
<p class="comment">
Computer Exercise C2.<br />
Use the data in <strong>wage1</strong> for this exercise.<br />
Use OLS to estimate the equation <span class="math display">\[log(wage) = \beta_0 + \beta_1*educ + \beta_2*exper + \beta_3*exper^2 + u\]</span> and report the results using the usual format. 1. Is <span class="math inline">\(exper^2\)</span> statistically significant at the 1% level?<br />
2. Using the approximation <span class="math inline">\(\% \Delta wage = 100(\beta_2 + 2\beta_3exper) \Delta exper\)</span><br />
3. find the approximate return to the fifth year of experience. What is the approximate return to the twentieth year of experience.<br />
4. At what value of exper does additional experience actually lower predicted <span class="math inline">\(log(wage)\)</span>? How many people have more experience in this sample?
</p>
<p class="comment">
Computer Exercise C9.<br />
The data set <strong>nbasal</strong> contains salary information and career statistics for 269 players in the National Basketball Association (NBA).<br />
1. Estimate a model relating points-per-game (<span class="math inline">\(points\)</span>) to years in the league (<span class="math inline">\(exper\)</span>), <span class="math inline">\(age\)</span>, and years played in college (<span class="math inline">\(coll\)</span>). Include a quadratic in exper; the other variables should appear in level form. Report the results in the usual way.<br />
2. Holding college years and age fixed, at what value of experience does the next year of experience actually reduce points-per-game? Does this make sense?<br />
3. Why do you think <span class="math inline">\(coll\)</span> has a negative and statistically significant coefficient? (Hint: NBA players can be drafted before finishing their college careers and even directly out of high school.)<br />
4. Add a quadratic in age to the equation. Is it needed? What does this appear to imply about the effects of age, once experience and education are controlled for?<br />
5. Now regress <span class="math inline">\(log(wage)\)</span> on <span class="math inline">\(points\)</span>, <span class="math inline">\(exper\)</span>, <span class="math inline">\(exper^2\)</span>, <span class="math inline">\(age\)</span>, and <span class="math inline">\(coll\)</span>. Report the results in the usual format.<br />
6.Test whether age and coll are jointly significant in the regression from part 5. What does this imply about whether age and education have separate effects on wage, once productivity and seniority are accounted for?
</p>
<p class="comment">
Computer Exercise C11.<br />
Use <strong>apple</strong> to verify some of the claims made in Section 6.3.<br />
1. Run the regression <span class="math inline">\(ecolbs\)</span> on <span class="math inline">\(ecoprc\)</span>, <span class="math inline">\(regprc\)</span> and report the results in the usual form, including the R-squared and adjusted R-squared. Interpret the coefficients on the price variables and comment on their signs and magnitudes.<br />
2. Are the price variables statistically significant? Report the p-values for the individual t-tests.<br />
3. What is the range of fitted values for <span class="math inline">\(ecolbs\)</span>? What fraction of the sample reports <span class="math inline">\(ecolbs=0\)</span>? Comment.<br />
4. Do you think the price variables together do a good job of explaining variation in <span class="math inline">\(ecolbs\)</span>? Explain.<br />
5. Add the variables <span class="math inline">\(faminc\)</span>, <span class="math inline">\(hhsize\)</span> (household size), <span class="math inline">\(educ\)</span>, and <span class="math inline">\(age\)</span> to the regression from part 1. Find the p-value for their joint significance. What do you conclude?<br />
6. Run separate simple regressions of <span class="math inline">\(ecolbs\)</span> on <span class="math inline">\(ecoprc\)</span> and then <span class="math inline">\(ecolbs\)</span> on <span class="math inline">\(regprc\)</span>. How do the simple regression coefficients compare with the multiple regression from part 1? Find the correlation coefficient between <span class="math inline">\(ecoprc\)</span> and <span class="math inline">\(regprc\)</span> to help explain your findings.
</p>
<p><strong>References</strong></p>
<p>Wooldridge, J. (2019). Introductory econometrics: a modern approach. Boston, MA: Cengage.</p>
<p>Heiss, F. (2016). Using R for introductory econometrics. Düsseldorf: Florian Heiss,CreateSpace.</p>
<hr />
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
