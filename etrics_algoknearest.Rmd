---
title: "K-nearest Neighbors Algorithm"
output: html_document
---


K-nearest neighbors (KNN) algorithm is a non-parametric supervised machine learning algorithm used for classification and regression. We will go over the basics of how it works and will use it to predict whether a unit belongs to a group given some information.

Imagine we have some initial observations with various attributes (for example, whether the person is married, wage, education, experience). When we obtain a set of attributes of a new individual (such as wage,  education, experience) we are trying to determine whether the individual is married or not. To do that, we compute the Euclidean distance between the attributes of the individual and the individuals already observed earlier. If the attributes of the new data point are closer to the married individuals' attributes, KNN will predict or classify that individual as married. You can read more about KNN here.

To run the knn() and npreg() commands we need to load the following libraries.
```{r, message=FALSE, warning=FALSE, cache=TRUE}
library(class)
library(np)
```

In the example below we will try to predict whether a person is married given the information on wage, education, work experience, and tenure using K-nearest neighbors algorithm in R, and compare the results with other predictive methods.
First, let's load the data.

```{r, message=FALSE, warning=FALSE, cache=TRUE}
data(wage1, package='wooldridge')
data1=wage1
summary(data1)

plot(factor(data1$married))
```

We see that in our sample we have both married and non-married individuals. The numerical data (wage, education, experience, tenure) needs to be normalized. Data normalization is a simple and useful tool in many applications. It converts the variables to a new scale from 0 to 1. When computing the Euclidean distances, it is important to have the same scale. It avoids one parameter being much larger and having a larger weight (effect) than other variables. We technically should not include variable "married" in the normalization function but since it is already normalized (that is between 0 and 1), it is not affected by the function.


```{r, message=FALSE, warning=FALSE, cache=TRUE}
normalizationfx=function(x) { (x-min(x))/(max(x)-min(x)) }
data2=as.data.frame(lapply(data1[,c("wage","educ","exper","tenure","married")], normalizationfx))
summary(data2)
```


We will need  a sample to train the algorithm or set-up rules which then will be used to predict the gender of the individual given the predictor variables. To train the algorithm, let's use 80% of the full data set. Our goal is to best predict the rest 20% of the data.


```{r, message=FALSE, warning=FALSE, cache=TRUE}
set.seed(123) #makes sure the results are replicable
sample1=sample(1:nrow(wage1), 0.80*nrow(wage1))
data3_train=data2[sample1,]
data3_test=data2[-sample1,]
```


First, we will arbitrarily start with a low value of k, that is, k=3. To run the knn algorithm, we can use already predefined function knn(). Otherwise, we could write ourselves (for example, see here).


```{r, message=FALSE, warning=FALSE, cache=TRUE}
Knn1=knn(data3_train[,c(1:4)], data3_test[,c(1:4)], cl=data3_train[,5], k=3)
Result1=cbind(data3_test,Knn1)[,c(5,6)]
Accuracy1=sum(Result1[,1]==Result1[,2])/nrow(Result1)
Table1=table(Knn1,data3_test[,5])
```


Let's try higher values of k. Increasing the value of k, in simple terms increases the size of the neighborhood in the multidimensional world.


```{r, message=FALSE, warning=FALSE, cache=TRUE}
Knn2=knn(data3_train[,c(1:4)], data3_test[,c(1:4)], cl=data3_train[,5], k=5)
Result2=cbind(data3_test,Knn2)[,c(5,6)]
Accuracy2=sum(Result2[,1]==Result2[,2])/nrow(Result2)
Table2=table(Knn2,data3_test[,5])
```


Again, let's see what happens if we increase k to 10.


```{r, message=FALSE, warning=FALSE, cache=TRUE}
Knn3=knn(data3_train[,c(1:4)], data3_test[,c(1:4)], cl=data3_train[,5], k=10)
Result3=cbind(data3_test,Knn3)[,c(5,6)]
Accuracy3=sum(Result3[,1]==Result3[,2])/nrow(Result3)
Table3=table(Knn3,data3_test[,5])
```


And one more time. Now let's set k=25.


```{r, message=FALSE, warning=FALSE, cache=TRUE}
Knn4=knn(data3_train[,c(1:4)], data3_test[,c(1:4)], cl=data3_train[,5], k=25)
Result4=cbind(data3_test,Knn4)[,c(5,6)]
Accuracy4=sum(Result4[,1]==Result4[,2])/nrow(Result4)
Table4=table(Knn4,data3_test[,5])
```


Finally, we can use all the neighbors, not only the few closest ones determined by k. Including more variables, as in a regression model, reduces the variance. However, we may end up missing some more intricate patterns happening in narrow ranges of data.


```{r, message=FALSE, warning=FALSE, cache=TRUE}
Knn5=knn(data3_train[,c(1:4)], data3_test[,c(1:4)], cl=data3_train[,5], use.all = TRUE)
Result5=cbind(data3_test,Knn5)[,c(5,6)]
Accuracy5=sum(Result5[,1]==Result5[,2])/nrow(Result5)
Table5=table(Knn5,data3_test[,5])
```


For comparative reasons, let's also estimate a linear, probit and non-parametric regression models. A linear regression model with a binary dependent variable is called a linear probability model. Let's take that if the resulting value is equal or over 0.5, we expect the individual to be married.


```{r, message=FALSE, warning=FALSE, cache=TRUE}
reg1=lm(married~wage+educ+exper+tenure, data=wage1[sample1,])
summary(reg1)
pred1=predict(reg1, newdata=wage1[-sample1,])
pred1a=pred1>=0.5
ResultReg1=data.frame(pred1a,wage1[-sample1,"married"])
AccuracyReg1=sum(ResultReg1[,1]==ResultReg1[,2])/nrow(ResultReg1);
TableReg1=table(pred1a,data3_test[,5])
```


Probit regression does not assume linear function as the values below 0 and above 1 are not logical, this model limits the possible values between 0 and 1. 


```{r, message=FALSE, warning=FALSE, cache=TRUE}
reg2 = glm(married~wage+educ+exper+tenure, family = binomial(link = probit), data = wage1[sample1,])
summary(reg2)
pred2=predict(reg2, newdata=wage1[-sample1,])
pred2a=pred2>=0.5
ResultReg2=data.frame(pred2a,wage1[-sample1,"married"])
AccuracyReg2=sum(ResultReg2[,1]==ResultReg2[,2])/nrow(ResultReg2);
TableReg2=table(pred2a,data3_test[,5])
```


Non-parametric regression does not assume any functional form of the regression. It requires significantly more time to run even for a small number of variables and a small data set like this one.


```{r, message=FALSE, warning=FALSE, cache=TRUE}
reg3=npreg(married~wage+educ+exper+tenure, regtype = "ll", bwmethod = "cv.aic", gradients = TRUE, data = wage1[sample1,])
summary(reg3)
pred3=predict(reg3, newdata=wage1[-sample1,])
pred3a=pred3>=0.5
ResultReg3=data.frame(pred3a,wage1[-sample1,"married"])
AccuracyReg3=sum(ResultReg3[,1]==ResultReg3[,2])/nrow(ResultReg3);
TableReg3=table(pred3a,data3_test[,5])
```


To compare the results of all the above predictions, we combine the tables into one table separated by a column of NAs. The first table for the first KNN estimation correctly predicts 17 unmarried people and correctly predicts 61 married people. However, 19 individuals were not married but we identified them as married. Similarly, 9 were actually married but our model predicted them not to be married. All in all, our first algorithm was 73.6% accurate with its predictions. As we increased the number of neighboring observations to be included in computation to k=5 and k=10, the prediction accuracy increased to 78.3%. If you look at the tables above, model 2 only predicted 3 people who were married as not married, and model 3 incorrectly predicted only two people as nor married.


Linear regression and non-parametric regression results yield very similar results, although slightly less accurate.


```{r, message=FALSE, warning=FALSE, cache=TRUE}
cbind(Table1,NA,Table2,NA,Table3,NA,Table4,NA,Table5,NA,TableReg1,NA,TableReg2,NA,TableReg3)
round(data.frame(Accuracy1,Accuracy2,Accuracy3,Accuracy4,Accuracy5,AccuracyReg1,AccuracyReg2, AccuracyReg3),3)
```


As we can see, our KNN models predicted who's married with an accuracy of almost 80%. As we acquire more data, we could expect our prediction accuracy to increase.

We could have generalized the algorithm to try all k's by using 'for' loop. For example, try the following code:


```{r, message=FALSE, warning=FALSE, cache=TRUE}
Accuracy=rep(0,100)
for (i in 1:100)
{
Knn5=knn(data3_train[,c(1:4)], data3_test[,c(1:4)], cl=data3_train[,5], k=i)
Result5=cbind(data3_test,Knn5)[,c(5,6)]
Accuracy[i]=sum(Result5[,1]==Result5[,2])/nrow(Result5)
}
plot(Accuracy, type="l")
max(Accuracy)
which(Accuracy==max(Accuracy))
```

---
