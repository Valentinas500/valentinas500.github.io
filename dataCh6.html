<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Practical Data Science With R. Chapter 6</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Valentinas Rudys</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="research.html">Research</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Teaching
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="teaching.html">Overview</a>
    </li>
    <li>
      <a href="etrics.html">Econometrics</a>
    </li>
    <li>
      <a href="macro.html">Macroeconomics</a>
    </li>
  </ul>
</li>
<li>
  <a href="programming.html">Programming</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="mailto:vrudys@fordham.edu">
    <span class="fas fa-envelope-o"></span>
     
    Contact me
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Practical Data Science With R. Chapter 6</h1>

</div>


<div id="chapter-6-choosing-and-evaluating-models" class="section level2">
<h2>Chapter 6: Choosing and Evaluating Models</h2>
<p>The model you end up choosing very much depends on the use of the model and the data. Typical problems that data scientists attempt to solve could be distinguished into these groups:</p>
<ul>
<li>Classification: assigning labels to datums</li>
<li>Scoring: assigning numerical values to datums</li>
<li>Grouping: discovering patterns and commonalities in data</li>
</ul>
<div id="classification" class="section level4">
<h4>Classification</h4>
<p>Classification is data categorization based on product attributes and/or text descriptions. classification is deciding how to assign (known) labels to an object. It is an example of what is called supervised learning: in order to learn how to classify objects, you need a dataset of objects that have already been classified (called the training set).</p>
</div>
<div id="scoring" class="section level4">
<h4>Scoring</h4>
<p>Regression or scoring is predicting the increase/decrease in dependent variable from a particular set of independent/explanatory variables. A regression model maps the different factors being measured into a numerical value: how much an increase in x is predicted to increase y from some baseline. Predicting the probability of an event (like belonging to a given class - say becoming unemployed) can also be considered scoring.</p>
</div>
<div id="grouping" class="section level4">
<h4>Grouping</h4>
<p>Classification and regression/scoring require that you have a training dataset of situations with known outcomes that you use to feed the model. However, in some situations, there is not (yet) a specific outcome that you want to predict. Instead, you may be searching for patterns and relationships in your data that will help you understand about how it all relates: for example, what products people buy together.</p>
<p>These situations lead to approaches called unsupervised learning: rather than predicting outputs based on inputs, the objective of unsupervised learning is to discover similarities and relationships in the data. Some common unsupervised tasks include:</p>
<ul>
<li>Clustering: Grouping similar objects together.</li>
<li>Association Rules: Discovering common behavior patterns, for example items that are always bought together, or library books that are always checked out together.</li>
</ul>
</div>
<div id="predicting-vs-forecasting" class="section level4">
<h4>Predicting vs Forecasting</h4>
<p>In everyday language, we tend to use the terms <em>prediction</em> and <em>forecast</em> interchangeably. Technically, to predict is to pick an outcome, such as “It will rain tomorrow,” and to forecast is to assign a probability: “There’s an 80% chance it will rain tomorrow.” That’s a big difference!</p>
</div>
<div id="evaluating-models" class="section level3">
<h3>Evaluating Models</h3>
<p>Being able to estimate how well your model performs with new data is very important. To evaluate the future model performance we split our data to two parts: training and test data. We use training data to feed the model during model development while test data is intended to inform us how will the model perform on new data.</p>
<div id="overfitting" class="section level4">
<h4>Overfitting</h4>
<p>An overfit model is a model that looks great on the training data but performs poorly on new data. What happens is that we train the model to much to fit the training data. In this case, the training error (a model’s prediction error on the data that it trained from) is going to be significantly smaller than generalation error (a model’s prediction error on new data). Not good! For a good model, the two errors should be similar. To avoid letting your model memorize the training data instead of learning generalizable rules, we should prefer simpler models which do in fact tend to generalize better.</p>
<p>Not splitting your data allows you to use more data to train and evaluate your model. However, you cannot should not evaluate the model on the data it has seen during its construction. When test data was used in training, we can expect an optimistic measurement bias. To avoid this, it is recommended to split your data into test and training data. Then, do all the modeling using the training data and only measuring the performance of your model using the test data at the very end.</p>
<p>If you split your data into training and test partitions, you may ask how should you split it. You want to balance the trade-off between keeping enough data to fit a good model, and having enough data to make good estimates of the model’s performance. Common splits are:</p>
<ul>
<li>50% training to 50% test</li>
<li>70% training to 30% test</li>
<li>80% training to 20% test</li>
</ul>
<p>In some cases, your data set is too small to be split. In this case, you can use a a more thorough partitioning scheme called <em>k-fold cross-validation</em>. The idea is to create multiple different subsets of training data and test data. In each case, make sure that they do not overlap. For example, data can be split into three non-overlapping partions, and the three partitions are arranged to form three test-train splits. For each split, a model is trained on the training set and then applied to the corresponding test set. See figure below.</p>
<p><img src="dataCh6_files/figure-html/unnamed-chunk-1-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="evaluating-classification-models" class="section level4">
<h4>Evaluating Classification Models</h4>
<p>A classification model is type of model that places examples into one of two or more categories. To measure classifier performance, we first must introduce a useful tool called the confusion matrix. It is the absolute most interesting summary of classifier performance. Confusion matrix is just a simple table that summarizes the classifier’s predictions against the actual known data categories.</p>
<p>See example using data on spam email below.</p>
<pre class="r"><code>spamD &lt;- read.table(&#39;R_data_files/spamD.tsv&#39;,header=T,sep=&#39;\t&#39;)
spamTrain &lt;- subset(spamD,spamD$rgroup &gt;= 10)
spamTest &lt;- subset(spamD,spamD$rgroup &lt; 10)
spamVars &lt;- setdiff(colnames(spamD), list(&#39;rgroup&#39;,&#39;spam&#39;))
spamFormula &lt;- as.formula(paste(&#39;spam == &quot;spam&quot;&#39;,
  paste(spamVars, collapse = &#39; + &#39;),sep = &#39; ~ &#39;))
  spamModel &lt;- glm(spamFormula,family = binomial(link = &#39;logit&#39;),
  data = spamTrain)
spamTrain$pred &lt;- predict(spamModel,newdata = spamTrain, type = &#39;response&#39;)
spamTest$pred &lt;- predict(spamModel,newdata = spamTest, type = &#39;response&#39;)

sample &lt;- spamTest[c(7,35,224,327), c(&#39;spam&#39;,&#39;pred&#39;)]
print(sample)</code></pre>
<pre><code>##          spam         pred
## 115      spam 0.9903246227
## 361      spam 0.4800498077
## 2300 non-spam 0.0006846551
## 3428 non-spam 0.0001434345</code></pre>
<pre class="r"><code>confmat_spam &lt;- table(truth = spamTest$spam,
prediction = ifelse(spamTest$pred &gt; 0.5,
&quot;spam&quot;, &quot;non-spam&quot;))

print(confmat_spam)</code></pre>
<pre><code>##           prediction
## truth      non-spam spam
##   non-spam      264   14
##   spam           22  158</code></pre>
<p>The confusion matrix is a table counting how often each combination of known outcomes (the truth) occurred in combination with each prediction type. For this example, 264 emails were correctly predicted to not be spam (true negatives), 158 were correctly identified as spam (trues positives), 14 were identified as spam even though they were not (false positives), and 22 were predicted to not be spam when in fact they were (false negatives).</p>
<p>Accuracy answers the question: “When the spam filter says this email is or is not spam, what’s the probability that it’s correct?”. Accuracy is defined as the number of items categorized correctly divided by the total number of items. That is done below.</p>
<pre class="r"><code>Accuracy = (confmat_spam[1,1] + confmat_spam[2,2]) / sum(confmat_spam)
Accuracy_error = 1 - Accuracy
print(c(Accuracy,Accuracy_error))</code></pre>
<pre><code>## [1] 0.92139738 0.07860262</code></pre>
<p>The error of around 8% is unacceptably high for a spam filter.</p>
<p>Precision answers the question “If the spam filter says this email is spam, what’s the probability that it’s really spam?”. Precision is defined as the ratio of true positives to predicted positives.</p>
<pre class="r"><code>Precision = confmat_spam[2,2] / (confmat_spam[2,2] + confmat_spam[1,2]) 
print(Precision)</code></pre>
<pre><code>## [1] 0.9186047</code></pre>
<p>In our email spam example, 92% precision means 8% of what was flagged as spam was in fact not spam.</p>
<p>The companion score to precision is recall. Recall answers recall the question “Of all the spam in the email set, what fraction did the spam filter detect?”. Recall is the ratio of true positives over all actual positives</p>
<pre class="r"><code>Recall= confmat_spam[2,2] / (confmat_spam[2,2] + confmat_spam[2,1])
print(Recall)</code></pre>
<pre><code>## [1] 0.8777778</code></pre>
<p>For our email spam filter recall is 88%, which means about 12% of the spam email we receive will still make it into our inbox.</p>
<p>Some people prefer to have just one number to compare all the different choices by. One such score is the . The F1 score measures a tradeoff F1 score between precision and recall. It is defined as the harmonic mean of the precision and recall.</p>
<pre class="r"><code>Precision &lt;- confmat_spam[2,2] / (confmat_spam[2,2]+ confmat_spam[1,2])
Recall &lt;- confmat_spam[2,2] / (confmat_spam[2,2] + confmat_spam[2,1])
F1 &lt;- 2 * Precision * Recall / (Precision + Recall)
print(F1)</code></pre>
<pre><code>## [1] 0.8977273</code></pre>
<p>Our spam filter with 0.93 precision and 0.88 recall has an F1 score of 0.90. F1 is one when a classifier has perfect precision and recall, and goes to zero for classifiers that have either very low precision or recall (or both). Quite often, increasing the precision of a classifier will also lower its recall: in this case, a pickier spam filter may also mark fewer real spam emails as spam, and allow it into your inbox. If the filter’s recall falls too low as its precision increases, this will result in a lower F1. This possibly means that you have traded off too much recall for better precision.</p>
<p>Sensitivity is also called the true positive rate and is exactly equal to recall. Specificity is also called the true negative rate: it is the ratio of true negatives to all negatives.</p>
<pre class="r"><code>Specificity = confmat_spam[1,1] / (confmat_spam[1,1] + confmat_spam[1,2])
print(Specificity)</code></pre>
<pre><code>## [1] 0.9496403</code></pre>
<p>One minus the specificity is also called the . False positive false positive rate rate answers the question What fraction of non-spam will the model classify as spam?. You want the false positive rate to be low (or the specificity to be high), and the sensitivity to also be high. Our spam filter has a specificity of about 0.95, which means that it will mark about 5% of non-spam email as spam.</p>
<p>Why have both precision/recall and sensitivity/specificity? Historically, these measures come from different fields, but each has advantages. Sensitivity/specificity is good for fields, like medicine, where it’s important to have an idea how well a classifier, test, or filter separates positive from negative instances independently of the distribution of the different classes in the population. But precision/recall give you an idea how well a classifier or filter will work on a specific population. If you want to know the probability that an email identified as spam is really spam, you have to know how common spam is in that person’s email box, and the appropriate measure is precision.</p>
</div>
<div id="evaluating-scoring-models" class="section level4">
<h4>Evaluating Scoring Models</h4>
<p>The most common goodness-of-fit measure is called root mean square error (RMSE). The RMSE is the square root of the average squared residuals (also called the mean squared error). RMSE answers the question How much is the predicted temperature typically off?</p>
<p>See an example studying how good are crickets at predicting temperature below.</p>
<pre class="r"><code>crickets &lt;- read.csv(&quot;R_data_files/crickets.csv&quot;)
cricket_model &lt;- lm(temperatureF ~ chirp_rate, data=crickets)
crickets$temp_pred &lt;- predict(cricket_model, newdata=crickets)
error_sq &lt;- (crickets$temp_pred - crickets$temperatureF)^2
RMSE &lt;- sqrt(mean(error_sq))
print(RMSE)</code></pre>
<pre><code>## [1] 3.564149</code></pre>
<p>Another important measure of fit is called R-squared (or R , or the coefficient of determination).</p>
<pre class="r"><code>error_sq &lt;- (crickets$temp_pred - crickets$temperatureF)^2
numerator &lt;- sum(error_sq)
delta_sq &lt;- (mean(crickets$temperatureF) - crickets$temperatureF)^2
denominator = sum(delta_sq)
R2 &lt;- 1 - numerator/denominator
print(R2)</code></pre>
<pre><code>## [1] 0.6974651</code></pre>
</div>
<div id="evaluating-probability-models" class="section level4">
<h4>Evaluating Probability Models</h4>
<p>Probability models are models that both decide if an item is in a given class and return an estimated probability (or confidence) of the item being in the class. The modeling techniques of logistic regression and decision trees are fairly famous for being able to return good probability estimates.</p>
<p>In order to turn a probability model into a classifier, you need to select a threshold: items that score higher than that threshold will be classified as spam, otherwise they are classified as non-spam. The easiest (and probably the most common) threshold for a probability model is 0.5, but the “best possible” classifier for a given probability model may require a different threshold.</p>
<p>When thinking about probability models, it’s useful to construct a double density plot.</p>
<pre class="r"><code>library(WVPlots)
DoubleDensityPlot(spamTest,
xvar = &quot;pred&quot;,
truthVar = &quot;spam&quot;,
title = &quot;Distribution of scores for spam filter&quot;)</code></pre>
<p><img src="dataCh6_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Double density plots can be useful when picking classifier thresholds, or the threshold score where the classifier switches from labeling an email as “non-spam” to “spam”.</p>
<p>The receiver operating characteristic curve (or ROC curve) is a popular alternative to the double density plot. For each different classifier we’d get by picking a different score threshold between spam and not-spam, we plot both the true positive (TP) rate and the false positive (FP) rate. The resulting curve represents every possible trade-off between true positive rate and false positive rate that is available for classifiers derived from this model.</p>
<pre class="r"><code>library(WVPlots)
ROCPlot(spamTest,
xvar = &#39;pred&#39;,
truthVar = &#39;spam&#39;,
truthTarget = &#39;spam&#39;,
title = &#39;Spam filter test performance&#39;)</code></pre>
<p><img src="dataCh6_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>library(sigr)
calcAUC(spamTest$pred, spamTest$spam==&#39;spam&#39;)</code></pre>
<pre><code>## [1] 0.9660072</code></pre>
<p>Log likelihood is a measure of how well the model’s predictions “match” the true class labels. It is a non-positive number, where a log likelihood of 0 means a perfect match: the model scores all the spam as being spam with a probability of one, and all the non-spam as having a probability zero of being spam. The larger the magnitude of the log likelihood, the worse the match. The log likelihood of a model’s prediction on a specific instance is the logarithm of the probability that the model assigns to the instance’s actual class.</p>
<pre class="r"><code>ylogpy &lt;- function(y, py) {
logpy = ifelse(py &gt; 0, log(py), 0)
y*logpy
}
y &lt;- spamTest$spam == &#39;spam&#39;
sum(ylogpy(y, spamTest$pred) +
ylogpy(1-y, 1-spamTest$pred))</code></pre>
<pre><code>## [1] -134.9478</code></pre>
<p>Another common measure when fitting probability models is the deviance. The deviance is defined as <span class="math inline">\(-2*(\log Likelihood-S)\)</span>, where S is a technical constant called “the log likelihood of the saturated model.” In most cases, the saturated model is a perfect model that returns probability 1 for items in the class and probability 0 for items not in the class (so S=0). The lower the deviance, the better the model.</p>
<p>An important variant of deviance is the Akaike information criterion (AIC). This is equivalent to <span class="math inline">\(deviance + 2*numberOfParameters\)</span> used in the model. The more parameters in the model, the more complex the model is; the more complex a model is, the more likely it is to overfit. Thus, AIC is deviance penalized for model complexity. When comparing models (on the same test set), you will generally prefer the model with the smaller AIC.</p>
</div>
<div id="local-interpretable-model-agnostic-explanations-lime" class="section level4">
<h4>Local Interpretable Model-Agnostic Explanations (LIME)</h4>
<p>In order to detect whether a model is really learning the concept, and not just data quirks, it’s not uncommon for domain experts to manually “sanity-check” a model by running some example cases through and looking at the answers. Generally, one would want to try a few typical cases, and a few extreme cases, just to see what happens. You can think of LIME as one form of automated sanity checking.</p>
<p>LIME produces an “explanation” of a model’s prediction on a specific datum. That is, LIME tries to determine which features of that datum contributed the most to the model’s decision about it. This helps data scientists attempt to understand the behavior of “black-box” machine learning models.</p>
<pre class="r"><code>iris1 &lt;- iris
iris1$class &lt;- as.numeric(iris1$Species == &quot;setosa&quot;)
set.seed(2345)
intrain &lt;- runif(nrow(iris1)) &lt; 0.75
train &lt;- iris1[intrain,]
test &lt;- iris1[!intrain,]
head(train)</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species class
## 1          5.1         3.5          1.4         0.2  setosa     1
## 2          4.9         3.0          1.4         0.2  setosa     1
## 3          4.7         3.2          1.3         0.2  setosa     1
## 4          4.6         3.1          1.5         0.2  setosa     1
## 5          5.0         3.6          1.4         0.2  setosa     1
## 6          5.4         3.9          1.7         0.4  setosa     1</code></pre>
<pre class="r"><code>source(&quot;R_data_files/lime_iris_example.R&quot;)
input &lt;- as.matrix(train[, 1:4])
model &lt;- fit_iris_example(input, train$class)</code></pre>
<pre><code>## [1]  train-logloss:0.454781+0.000056 test-logloss:0.454951+0.001252 
## [11] train-logloss:0.032154+0.000045 test-logloss:0.032292+0.001016 
## [21] train-logloss:0.020894+0.000931 test-logloss:0.021263+0.001448 
## [31] train-logloss:0.020881+0.000932 test-logloss:0.021271+0.001580 
## [41] train-logloss:0.020881+0.000932 test-logloss:0.021274+0.001597 
## [51] train-logloss:0.020881+0.000932 test-logloss:0.021274+0.001599 
## [61] train-logloss:0.020881+0.000932 test-logloss:0.021274+0.001599 
## [71] train-logloss:0.020881+0.000932 test-logloss:0.021274+0.001599 
## [81] train-logloss:0.020881+0.000932 test-logloss:0.021274+0.001599 
## [91] train-logloss:0.020881+0.000932 test-logloss:0.021274+0.001599 
## [100]    train-logloss:0.020881+0.000932 test-logloss:0.021274+0.001599 
## [11:40:42] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.</code></pre>
<pre class="r"><code>predictions &lt;- predict(model, newdata=as.matrix(test[,1:4]))
teframe &lt;- data.frame(isSetosa = ifelse(test$class == 1,
&quot;setosa&quot;,
&quot;not setosa&quot;),
pred = ifelse(predictions &gt; 0.5,
&quot;setosa&quot;,
&quot;not setosa&quot;))
with(teframe, table(truth=isSetosa, pred=pred))</code></pre>
<pre><code>##             pred
## truth        not setosa setosa
##   not setosa         25      0
##   setosa              0     11</code></pre>
<pre class="r"><code>library(lime)
explainer &lt;- lime(train[,1:4],
model = model,
bin_continuous = TRUE,
n_bins = 10)

(example &lt;- test[5, 1:4, drop=FALSE])</code></pre>
<pre><code>##    Sepal.Length Sepal.Width Petal.Length Petal.Width
## 30          4.7         3.2          1.6         0.2</code></pre>
<pre class="r"><code>test$class[5]</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>round(predict(model, newdata = as.matrix(example)))</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>explanation &lt;- lime::explain(example,
explainer,
n_labels = 1,
n_features = 4)

(example &lt;- test[c(13, 24), 1:4])</code></pre>
<pre><code>##     Sepal.Length Sepal.Width Petal.Length Petal.Width
## 58           4.9         2.4          3.3         1.0
## 110          7.2         3.6          6.1         2.5</code></pre>
<pre class="r"><code>test$class[c(13,24)]</code></pre>
<pre><code>## [1] 0 0</code></pre>
<pre class="r"><code>round(predict(model, newdata=as.matrix(example)))</code></pre>
<pre><code>## [1] 0 0</code></pre>
<pre class="r"><code>explanation &lt;- explain(example,
explainer,
n_labels = 1,
n_features = 4,
kernel_width = 0.5)
plot_features(explanation)</code></pre>
<p><img src="dataCh6_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p><strong>References</strong></p>
<p>Zumel, N., &amp; Mount, J. (2014). Practical Data Science With R. Manning Publications Co.</p>
<hr />
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
